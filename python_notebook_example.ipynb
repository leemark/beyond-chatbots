{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv requests beautifulsoup4 openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from google.colab import userdata\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "\n",
    "# Constants\n",
    "USER_PROMPT = \"How much is tuition at Colorado College in 2024-25?\"\n",
    "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
    "BRAVE_API_KEY = userdata.get(\"BRAVE_API_KEY\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def call_gpt4_turbo(messages):\n",
    "    \"\"\"\n",
    "    Make a call to the GPT-4 Turbo model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def rephrase_query(text):\n",
    "    \"\"\"\n",
    "    Use GPT-4 Turbo to rephrase the user's query into a search-friendly format.\n",
    "    This can help in getting more relevant search results.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Rephrase the following as a concise search query:\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    return call_gpt4_turbo(messages)\n",
    "\n",
    "def search_urls(query):\n",
    "    \"\"\"\n",
    "    Use Brave Search API to find relevant URLs based on the query.\n",
    "    Returns the top 5 URLs from the search results.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.search.brave.com/res/v1/web/search?q={query}\"\n",
    "    headers = {\"X-Subscription-Token\": BRAVE_API_KEY}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    results = response.json()\n",
    "    return [result['url'] for result in results.get('web', {}).get('results', [])][:5]\n",
    "\n",
    "def scrape_and_parse(urls):\n",
    "    \"\"\"\n",
    "    Scrape content from the given URLs and parse it using BeautifulSoup.\n",
    "    Returns the text content and source URLs.\n",
    "    \"\"\"\n",
    "    contents = []\n",
    "    sources = []\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"}\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            contents.append(soup.get_text())\n",
    "            sources.append(url)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {str(e)}\")\n",
    "\n",
    "    return contents, sources\n",
    "\n",
    "def embed_into_chroma_db(contents, sources, collection_name):\n",
    "    \"\"\"\n",
    "    Embed the scraped content into a Chroma database for efficient searching.\n",
    "    Each document is stored with its source URL as metadata.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient()\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    for content, source in zip(contents, sources):\n",
    "        collection.add(\n",
    "            documents=[content],\n",
    "            metadatas=[{\"source\": source}],\n",
    "            ids=[str(uuid.uuid4())]\n",
    "        )\n",
    "\n",
    "def query_chroma_db(query, collection_name, top_k=5):\n",
    "    \"\"\"\n",
    "    Query the Chroma database to find the most relevant documents for the given query.\n",
    "    Returns the top k documents and their sources.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient()\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    return results[\"documents\"][0], [m[\"source\"] for m in results[\"metadatas\"][0]]\n",
    "\n",
    "def generate_response(context, query):\n",
    "    \"\"\"\n",
    "    Use GPT-4 Turbo to generate a response based on the context and query.\n",
    "    The context is the relevant information retrieved from the Chroma database.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer the user's query accurately and concisely.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuery: {query}\\n\\nResponse:\"}\n",
    "    ]\n",
    "    return call_gpt4_turbo(messages)\n",
    "\n",
    "def main():\n",
    "    # Step 1: Rephrase the user's query\n",
    "    rephrased_query = rephrase_query(USER_PROMPT)\n",
    "    print(f\"Rephrased Query: {rephrased_query}\")\n",
    "\n",
    "    # Step 2: Search for relevant URLs\n",
    "    urls = search_urls(rephrased_query)\n",
    "\n",
    "    # Step 3: Scrape and parse content from the URLs\n",
    "    contents, sources = scrape_and_parse(urls)\n",
    "\n",
    "    # Step 4: Embed the scraped content into the Chroma database\n",
    "    embed_into_chroma_db(contents, sources, \"colorado_college_tuition\")\n",
    "\n",
    "    # Step 5: Query the Chroma database for relevant information\n",
    "    relevant_docs, relevant_sources = query_chroma_db(rephrased_query, \"colorado_college_tuition\")\n",
    "\n",
    "    # Step 6: Prepare the context for the response generation\n",
    "    context = \"\\n\".join([f\"Source: {source}\\nContent: {doc}\" for doc, source in zip(relevant_docs, relevant_sources)])\n",
    "\n",
    "    # Step 7: Generate the final response\n",
    "    response = generate_response(context, rephrased_query)\n",
    "\n",
    "    # Step 8: Print the results\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print(\"\\nRelevant Sources:\")\n",
    "    print(\"\\n\".join(relevant_sources))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
